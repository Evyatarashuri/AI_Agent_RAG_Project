# README.md

# ğŸ§  AI Academic Research Assistant

An intelligent agent that answers questions based on user-uploaded academic PDFs using Retrieval-Augmented Generation (RAG) and a locally hosted LLM via Ollama.

---

## ğŸš€ Features

- ğŸ“„ Upload and analyze multiple research PDFs
- ğŸ” Semantic search over document content
- ğŸ¤– Generate context-aware answers with source citations
- ğŸ§  Local LLM powered by Ollama (e.g. LLaMA2, Mistral)
- ğŸ“¦ Fully containerized with Docker and Docker Compose
- ğŸ“ Interactive API docs via Swagger
- ğŸ§¾ MongoDB-based query logging with metadata

---

## âš™ï¸ Tech Stack

| Component         | Tech Used            |
|------------------|----------------------|
| Backend API       | Flask + Pydantic     |
| Vector Database   | ChromaDB             |
| Embeddings        | nomic-embed-text / Ollama |
| Language Model    | Ollama (LLaMA2, etc) |
| Storage & Logs    | MongoDB + Mongo Express |
| Containerization  | Docker, Docker Compose |
| Docs UI           | Swagger/OpenAPI      |

---

## ğŸ“ Project Structure

```bash
project_root/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ modelfile/
â”‚   â””â”€â”€ Modelfile
â”œâ”€â”€ static/
â”‚   â””â”€â”€ swagger.json
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ database/
â”‚   â””â”€â”€ rag/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ§ª Quick Start (Local with Docker)

```bash
# 1. Build and start all services
$ docker-compose up --build

# 2. Visit the API:
#    - POST /papers (upload PDFs)
#    - POST /query  (ask questions)
#    - GET  /docs   (Swagger UI)
```

---

## ğŸ“Œ Status
âœ… Core functionality complete: upload â†’ embed â†’ query â†’ answer â†’ log.
ğŸ›  In progress: refining Swagger UI and adding test coverage.

---
